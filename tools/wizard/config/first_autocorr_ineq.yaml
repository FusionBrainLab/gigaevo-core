# First Autocorrelation Inequality Problem Configuration
# Example configuration for the wizard scaffolding system
#
# This is based on the alphaevolve_math_problems/first_autocorr_ineq problem.

name: "first_autocorr_ineq"
description: "First Autocorrelation Inequality Problem - finding optimal non-negative function to minimize upper bound for constant C1"

# Function signatures
entrypoint:
  params: []
  returns: "(f_values, c1_achieved, loss, n_points) where f_values is (n_points,) NumPy array, c1_achieved is float, loss is float, n_points is int"
  inputs: null

validation:
  params: ["f_values", "c1_achieved", "loss", "n_points"]
  returns: "dict with metrics: c1, combined_score, loss, n_points, is_valid"
  inputs: "f_values: (n_points,) array from entrypoint(), c1_achieved: float, loss: float, n_points: int"

# Metrics (is_valid is auto-generated, don't include it)
metrics:
  c1:
    description: "The upper bound for constant C1 found by the program (PRIMARY OBJECTIVE - minimize)"
    decimals: 8
    is_primary: false
    higher_is_better: false
    lower_bound: 0.0
    upper_bound: 10.0
    include_in_prompts: true
    significant_change: 0.00000001
    sentinel_value: 10.0
  combined_score:
    description: "Progress toward benchmark: 1.5052939684401607 / c1 (PRIMARY OBJECTIVE - maximize, > 1 means new record)"
    decimals: 8
    is_primary: true
    higher_is_better: true
    lower_bound: 0.0
    upper_bound: 2.0
    include_in_prompts: true
    significant_change: 0.00000001
    sentinel_value: 0.0
  loss:
    description: "Loss value used in minimization"
    decimals: 8
    is_primary: false
    higher_is_better: false
    lower_bound: 0.0
    upper_bound: 10.0
    include_in_prompts: true
    significant_change: 0.00000001
    sentinel_value: 10.0
  n_points:
    description: "Number of points used in the discretization"
    decimals: 0
    is_primary: false
    higher_is_better: false
    lower_bound: 1
    upper_bound: 10000
    include_in_prompts: true
    significant_change: 1
    sentinel_value: 0

# Task description
task_description:
  objective: |
    TASK DEFINITION - FIRST AUTOCORRELATION INEQUALITY PROBLEM

    Problem Type: Functional Analysis and Numerical Optimization
    Challenge: Find a non-negative function f: R -> R that minimizes the upper bound of the constant C1 in the inequality:
    max_{-1/2<=t<=1/2} f*f(t) >= C1 (integral_{-1/4}^{1/4} f(x) dx)^2
    where f*f(t) = integral f(t-x)f(x) dx is the autoconvolution.

    PROBLEM COMPLEXITY

    This is a constrained optimization problem in function space with non-negativity constraints. The objective involves finding the maximum of the autoconvolution over a specific interval, normalized by the square of the integral. The search space is high-dimensional (determined by discretization), and the problem requires balancing the autoconvolution maximum with the integral constraint. Effective solutions must use sophisticated optimization techniques to navigate the non-convex landscape while maintaining strict non-negativity throughout the domain.

    You are an expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to evolve and optimize a Python script to find the optimal function that minimizes the upper bound of the constant C1.

    OBJECTIVE:
    Implement a Python function that returns a non-negative function f and its corresponding C1 bound, such that the bound is minimized (or equivalently, combined_score = benchmark / c1 is maximized).

    - Output: (f_values, c1_achieved, loss, n_points) where:
      * f_values: (n_points,) NumPy array representing the function values on the discretized domain [-1/4, 1/4]
      * c1_achieved: float representing the computed upper bound
      * loss: float representing the loss value used in minimization
      * n_points: int representing the number of discretization points
    - Fitness: combined_score = 1.5052939684401607 / c1 (PRIMARY OBJECTIVE - maximize)
    - Goal: Beat the AlphaEvolve state-of-the-art result of c1 = 1.5052939684401607 (combined_score > 1.0)
    - Performance metrics: combined_score (primary), c1, loss, n_points
    - Technical requirements: Determinism (fixed random seeds), error handling, constraint satisfaction, numerical precision

    PROBLEM SETUP:
    - Domain: [-1/4, 1/4] interval for function definition
    - Autoconvolution domain: [-1/2, 1/2] for evaluation
    - Function range: f(x) >= 0 for all x (hard constraint)
    - Discretization: n_points equally spaced points in [-1/4, 1/4]
    - Step size: dx = 0.5 / n_points

    CONSTRAINTS:
    - f(x) must be non-negative everywhere: f(x) >= 0 (hard constraint)
    - Integral of f over [-1/4, 1/4] must be positive: integral f dx > 0 (hard constraint)
    - The C1 bound is computed as: max_{t in [-1/2,1/2]} (f*f)(t) / (integral f dx)^2
    - Numerical precision matters for autoconvolution and integral computations

    MATHEMATICAL FORMULATION:
    Given: Discretized domain [-1/4, 1/4] with n_points equally-spaced grid points.
    Objective: Minimize min_{t in [-1/2,1/2]} (f*f)(t) / (integral f dx)^2 over all non-negative functions f.
    Discretization: Use finite differences and discrete convolution to approximate integrals and autoconvolution.

  hints:
    - "Non-negativity violation: f(x) values negative -> use ReLU or clipping to enforce constraint"
    - "Integral too small: Function integral near zero -> avoid degenerate solutions"
    - "Suboptimal bound: Autoconvolution maximum not minimized -> explore different function shapes"
    - "Discretization: Too few points may miss optimal solutions, too many may slow computation"
    - "Optimization strategy: Consider gradient-based methods (JAX, PyTorch) with non-negativity constraints"
    - "Autoconvolution computation: Use FFT for efficient convolution calculation"
    - "Use program insights to identify patterns affecting the autoconvolution maximum"
    - "Leverage lineage outcomes to guide optimization decisions"

# Optional features
add_context: false
add_helper: true

# Initial programs
initial_programs:
  - name: "jax_optimizer"
    description: "JAX-based gradient optimization with ReLU non-negativity constraint"
  - name: "uniform"
    description: "Uniform function with proper normalization"
  - name: "step_function"
    description: "Step function with optimization"
